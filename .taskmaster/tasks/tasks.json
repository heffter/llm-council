{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Provider abstraction, env-driven configuration, and provider:model parsing",
        "description": "Introduce a unified LLM provider abstraction that supports provider:model notation, env-driven per-role configuration (council + chairman + optional research role), and fail-fast validation of models/keys at startup.",
        "details": "Implementation outline:\n\n- **Tech choices & libraries**\n  - Backend language: follow existing codebase; if Node.js/TypeScript, use `node-fetch@3` or `undici@6` for HTTP.\n  - For config: use `zod@3` or `joi@17` to validate env/config objects.\n  - For type-safe env access: `dotenv@16` + a thin wrapper.\n\n- **Config & notation**\n  - Support model identifiers of form `\"provider:model\"`, e.g. `\"openai:gpt-4.1\"`, `\"anthropic:claude-3-5-sonnet\"`, `\"gemini:gemini-2.0-pro\"`, `\"perplexity:sonar-pro\"`, `\"openrouter:gpt-4.1\"`.\n  - Implement a parser:\n    ```ts\n    type ProviderId = 'openai' | 'anthropic' | 'gemini' | 'perplexity' | 'openrouter';\n\n    interface ParsedModelId {\n      provider: ProviderId;\n      model: string; // provider-native model ID\n    }\n\n    function parseProviderModel(id: string): ParsedModelId {\n      const [provider, model] = id.split(':');\n      if (!provider || !model) throw new Error(`Invalid provider:model string: ${id}`);\n      if (!['openai','anthropic','gemini','perplexity','openrouter'].includes(provider)) {\n        throw new Error(`Unsupported provider: ${provider}`);\n      }\n      return { provider: provider as ProviderId, model };\n    }\n    ```\n\n- **Env-driven per-role config**\n  - Define env vars for council and chairman roles, plus optional research role, e.g.:\n    - `COUNCIL_MODELS=openai:gpt-4.1,anthropic:claude-3-5-sonnet`\n    - `CHAIRMAN_MODEL=anthropic:claude-3-5-sonnet`\n    - `RESEARCH_MODEL=perplexity:sonar-pro` (optional)\n  - Map providers to key env vars:\n    - `OPENAI_API_KEY`\n    - `ANTHROPIC_API_KEY`\n    - `GEMINI_API_KEY`\n    - `PERPLEXITY_API_KEY`\n    - `OPENROUTER_API_KEY`\n  - On startup, parse env, build a `ProviderConfig` registry:\n    ```ts\n    interface ProviderConfig {\n      apiKey: string;\n      baseUrl: string;\n      timeoutMs: number;\n    }\n\n    const providerConfigs: Record<ProviderId, ProviderConfig | undefined> = {\n      openai: process.env.OPENAI_API_KEY ? {\n        apiKey: process.env.OPENAI_API_KEY!,\n        baseUrl: process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1',\n        timeoutMs: 60000,\n      } : undefined,\n      // ... similarly for others\n    };\n    ```\n\n- **Provider abstraction**\n  - Define an interface similar to patterns used in multi-LLM platforms[3]:\n    ```ts\n    interface LLMProviderClient {\n      complete(opts: {\n        model: string;\n        messages: { role: 'system'|'user'|'assistant'; content: string }[];\n        temperature?: number;\n        maxTokens?: number;\n      }): Promise<AsyncIterable<string> | string>; // depending on streaming\n    }\n    ```\n  - Implement concrete clients:\n    - **OpenAI**: use current `chat.completions` API (`gpt-4.1`, `gpt-4.1-mini`, `o3-mini`) with `Authorization: Bearer`.\n    - **Anthropic**: use `/v1/messages` for Claude 3.x (e.g., `claude-3-5-sonnet-latest`), `x-api-key` header.\n    - **Gemini**: use `v1beta/models/{model}:streamGenerateContent` or non-streaming `generateContent`.\n    - **Perplexity**: use `/chat/completions` compatible with OpenAI format where available.\n    - **OpenRouter**: proxy OpenAI-compatible requests to `https://openrouter.ai/api/v1` with `Authorization: Bearer`.\n\n- **Routing by role**\n  - In the council orchestration layer, map logical roles to parsed models:\n    ```ts\n    interface RoleConfig {\n      id: string; // 'chairman' | 'council' | 'research'\n      modelId: string; // provider:model\n    }\n\n    const councilModels = process.env.COUNCIL_MODELS?.split(',') ?? [];\n    const chairmanModel = process.env.CHAIRMAN_MODEL;\n    const researchModel = process.env.RESEARCH_MODEL; // optional\n    ```\n  - Expose a function:\n    ```ts\n    async function callRoleLLM(role: 'council'|'chairman'|'research', payload: PromptPayload) {\n      const modelId = selectModelForRole(role); // e.g., round-robin across council list\n      const { provider, model } = parseProviderModel(modelId);\n      const client = getProviderClient(provider);\n      if (!client) throw new Error(`Provider not configured: ${provider}`);\n      return client.complete({ model, messages: payload.messages });\n    }\n    ```\n\n- **Fail-fast validation**\n  - On application startup:\n    - Parse all configured provider:model IDs.\n    - For each provider referenced by any role, assert corresponding API key env var is present; if not, throw and log explicit message: e.g., `Missing OPENAI_API_KEY for configured model openai:gpt-4.1`.\n    - For optional **research role**: if `RESEARCH_MODEL` set but provider key missing, log a warning and disable research role; otherwise keep research role disabled by default.\n  - Distinguish between **hard errors** (core roles) vs **optional** (research) when exiting.\n\n- **Optional research role (Perplexity)**\n  - Implement a separate orchestration path that calls Perplexity (or other provider) for research/ranking/title/aux prompts.\n  - If research role is requested but not configured (no model or no key), the SSE stream to the frontend must emit a typed error event (see Task 4) indicating research functionality is unavailable.\n\n- **Security & robustness**\n  - Never log full API keys; at most log provider names and last 4 chars of keys when debugging.\n  - Keep provider-specific request/response normalization inside the abstraction layer to minimize leakage of provider quirks into the main app.\n\n- **Future-proofing**\n  - Store model names as plain strings; do not hard-code the list of allowed model strings beyond provider type checking, so upgrades to `gpt-5` or new Claude versions are configuration-only changes.\n\n<info added on 2025-12-09T15:31:27.345Z>\n**Implementation completed (commit 71891d7):**\n\n- **File structure created:**\n  - `backend/providers/parser.py` - Provider:model notation parser with validation\n  - `backend/providers/base.py` - Abstract `LLMProviderClient` base class\n  - `backend/providers/openai_provider.py` - OpenAI GPT models implementation\n  - `backend/providers/anthropic_provider.py` - Claude models with proper message format\n  - `backend/providers/gemini_provider.py` - Google Gemini with content/parts conversion\n  - `backend/providers/perplexity_provider.py` - OpenAI-compatible implementation\n  - `backend/providers/openrouter_provider.py` - Multi-provider proxy\n  - `backend/providers/registry.py` - Provider registry with env-based config and caching\n  - `backend/llm_client.py` - High-level interface replacing openrouter.py\n\n- **Configuration updates:**\n  - Updated `backend/config.py` to parse `COUNCIL_MODELS`, `CHAIRMAN_MODEL`, `RESEARCH_MODEL`\n  - Added comma-separated model list support\n  - Implemented startup validation with fail-fast behavior for missing API keys\n  - Created comprehensive `.env.example` with provider:model notation examples\n\n- **Integration points:**\n  - Updated `backend/council.py` to use new provider abstraction\n  - Added startup validation hook in `backend/main.py`\n  - All providers use async httpx with configurable timeouts\n  - Research model optional with graceful degradation\n\n- **Technical implementation notes:**\n  - Used Python instead of Node.js/TypeScript as indicated by file extensions\n  - All HTTP calls use async httpx instead of node-fetch/undici\n  - Error handling converts provider exceptions to descriptive messages\n  - Provider-specific quirks isolated within abstraction layer\n  - Client caching prevents duplicate provider instantiation\n</info added on 2025-12-09T15:31:27.345Z>",
        "testStrategy": "- **Unit tests**\n  - Test `parseProviderModel` with valid/invalid strings, unknown providers, missing parts.\n  - Test env parsing for council/chairman/research models, including multiple council models and no research model.\n  - Mock env vars to verify startup validation fails when a configured provider lacks its API key, and passes when present.\n  - For each provider client (OpenAI, Anthropic, Gemini, Perplexity, OpenRouter), mock HTTP and assert correct URLs, headers, and payload formats are used.\n  - Test `callRoleLLM` routes to the correct provider and model based on role and env.\n\n- **Integration tests**\n  - With fake or sandbox keys, issue a full council request and ensure each configured role calls the correct provider endpoint.\n  - Configure an invalid provider string and verify the service refuses to start with a clear error.\n  - Configure `RESEARCH_MODEL` without its API key and verify research role is marked unavailable and surfaces a structured error when invoked.\n\n- **Negative-path tests**\n  - Attempt to call a role with no configured model and assert an immediate, descriptive error is thrown and translated to SSE error events later.\n  - Simulate provider HTTP 401/403 and ensure they are surfaced as provider-specific failures (to be wired into SSE in Task 4).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T15:31:29.483Z"
      },
      {
        "id": "2",
        "title": "Storage safety: UUID conversation IDs, path sanitization, and response size capping",
        "description": "Harden conversation storage by enforcing UUID IDs at API boundary, sanitizing file paths to remain under data/conversations/, and truncating oversized responses before writing to disk.",
        "details": "Implementation outline:\n\n- **UUID conversation IDs at API boundary**\n  - Standardize conversation identifier as **UUID v4** strings.\n  - Use a robust UUID library: e.g., `uuid@9` (`import { validate, version } from 'uuid';`).\n  - On any API endpoint that accepts a conversation ID (read/write/list/delete), validate:\n    ```ts\n    function assertValidConversationId(id: string) {\n      if (!validate(id) || version(id) !== 4) {\n        throw new BadRequestError('conversation_id must be a valid UUID v4');\n      }\n    }\n    ```\n  - Reject non-UUIDs before any filesystem or DB access and return HTTP 400 with a clear JSON error.\n\n- **Safe path construction**\n  - Ensure all conversation files are stored under `data/conversations/`.\n  - Use a single helper to resolve the path and prevent traversal:\n    ```ts\n    import path from 'node:path';\n\n    const CONVERSATION_ROOT = path.resolve('data/conversations');\n\n    function getConversationFilePath(conversationId: string): string {\n      assertValidConversationId(conversationId);\n      const safeName = `${conversationId}.json`;\n      const fullPath = path.resolve(CONVERSATION_ROOT, safeName);\n      if (!fullPath.startsWith(CONVERSATION_ROOT + path.sep)) {\n        throw new Error('Invalid conversation path');\n      }\n      return fullPath;\n    }\n    ```\n  - Create directories on startup if missing (using `fs.mkdir(CONVERSATION_ROOT, { recursive: true })`).\n\n- **Response size capping / truncation**\n  - Define a maximum stored response size in bytes or characters, configurable via env, e.g. `MAX_STORED_RESPONSE_BYTES` with a sensible default (e.g., 256KB–1MB).\n  - Before persisting an LLM turn, apply truncation to large assistant messages:\n    ```ts\n    const MAX_BYTES = Number(process.env.MAX_STORED_RESPONSE_BYTES ?? 262144);\n\n    function truncateForStorage(text: string): string {\n      const encoder = new TextEncoder();\n      const data = encoder.encode(text);\n      if (data.byteLength <= MAX_BYTES) return text;\n      const truncated = data.slice(0, MAX_BYTES);\n      const decoder = new TextDecoder('utf-8', { fatal: false });\n      return decoder.decode(truncated) + '\\n[TRUNCATED]';\n    }\n    ```\n  - Apply truncation only to the **persisted representation**; stream full content to the client as permitted by other limits.\n\n- **File format & schema**\n  - Store each conversation as JSON with a clear schema, e.g.:\n    ```json\n    {\n      \"id\": \"<uuid-v4>\",\n      \"createdAt\": \"ISO-8601\",\n      \"updatedAt\": \"ISO-8601\",\n      \"messages\": [\n        {\"id\":\"uuid-v4\",\"role\":\"user\",\"content\":\"...\",\"createdAt\":\"...\"},\n        {\"id\":\"uuid-v4\",\"role\":\"assistant\",\"content\":\"...\",\"createdAt\":\"...\"}\n      ]\n    }\n    ```\n  - Validate this structure when reading; if corrupted, handle gracefully (return 500 + message) rather than crashing.\n\n- **Operational safeguards**\n  - Keep `data/` in `.gitignore` (Task 6 will update docs, but implementation must not rely on Git only).\n  - Optionally add a background script to detect and log oversized or corrupted files.\n\n- **Performance considerations**\n  - Prefer streaming writes or append strategies only after truncation; avoid holding extremely large responses in memory when not needed.\n  - Use asynchronous file operations (`fs.promises`) to keep the event loop responsive.\n\n<info added on 2025-12-09T15:42:16.174Z>\n**Implementation Status: COMPLETE**\n\nAll core storage safety features have been successfully implemented in Python backend:\n\n- **Storage utilities module** (`backend/storage_utils.py`):\n  - `validate_conversation_id()` - UUID v4 validation with canonical string format enforcement\n  - `get_safe_conversation_path()` - Path traversal prevention using Path.resolve() and relative_to()\n  - `truncate_for_storage()` - UTF-8 safe byte-based truncation with [TRUNCATED] marker\n  - Custom exceptions: `InvalidConversationIdError`, `PathTraversalError`\n\n- **Storage layer hardening** (`backend/storage.py`):\n  - UUID v4 validation applied to all conversation_id parameters before file operations\n  - Safe path construction integrated into `get_conversation_path()`\n  - Graceful corrupted file handling in `get_conversation()` - returns None instead of crashing\n  - Response truncation implemented in `add_assistant_message()` for all 3 processing stages\n  - `MAX_STORED_RESPONSE_BYTES` environment configuration (default 256KB)\n\n- **API boundary protection** (`backend/main.py`):\n  - Exception handling for `InvalidConversationIdError` and `PathTraversalError`\n  - HTTP 400 responses for invalid UUIDs and path traversal attempts\n  - Applied to all conversation endpoints: `get_conversation`, `send_message`, `send_message_stream`\n\n- **Configuration documentation**:\n  - Added `MAX_STORED_RESPONSE_BYTES` to `.env.example` with usage documentation\n\n**Security posture achieved**:\n- UUID v4 validation prevents injection attacks and ensures canonical identifier format\n- Path resolution with relative_to() check prevents directory traversal attacks\n- Response size capping protects against disk exhaustion from oversized LLM outputs\n- Graceful JSON corruption handling prevents application crashes\n- All validation occurs at API boundary before any storage operations\n\nImplementation completed and committed in 444c774.\n</info added on 2025-12-09T15:42:16.174Z>",
        "testStrategy": "- **Unit tests**\n  - Test `assertValidConversationId` with valid/invalid UUIDs, ensuring non-v4 or malformed IDs are rejected.\n  - Test `getConversationFilePath`:\n    - For a valid UUID, path is under `data/conversations/` and ends with `.json`.\n    - For crafted traversal IDs (e.g., `\"../evil\"`), ensure validation rejects before path resolution.\n  - Test `truncateForStorage` with responses below and above threshold, verifying byte-based truncation and `[TRUNCATED]` suffix.\n\n- **Integration tests**\n  - Create a conversation via API using a valid UUID; then verify that a JSON file exists at `data/conversations/<uuid>.json`.\n  - Try to access a conversation with `conversation_id=123` or `\"../../etc/passwd\"` and assert HTTP 400 with appropriate error body.\n  - Simulate a very large assistant response and check that:\n    - The client receives the full stream (if allowed by other limits).\n    - The stored file contains a truncated version with the marker.\n\n- **Resilience tests**\n  - Manually corrupt a stored JSON file and verify read operations handle it by returning an error without crashing the process.\n  - Benchmark typical write paths to confirm truncation and validation do not significantly degrade throughput for normal-sized conversations.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T15:42:18.721Z"
      },
      {
        "id": "3",
        "title": "Auth and rate limiting for write endpoints",
        "description": "Add optional shared-secret authentication on write endpoints and a simple configurable per-token/IP rate limiter, disabled by default for local use.",
        "details": "Implementation outline:\n\n- **Identify write endpoints**\n  - Treat endpoints that create/modify data as **write** (e.g., start conversation, send message, delete conversation, rename, etc.).\n  - Read-only (fetch conversation, health, config) remain unauthenticated unless otherwise needed.\n\n- **Shared secret header auth**\n  - Introduce env-configured shared secret, e.g., `SHARED_WRITE_TOKEN`.\n  - Choose a header name like `X-Shared-Token`.\n  - Middleware pseudo-code:\n    ```ts\n    const WRITE_TOKEN = process.env.SHARED_WRITE_TOKEN;\n\n    function sharedSecretMiddleware(req, res, next) {\n      if (!WRITE_TOKEN) return next(); // feature disabled\n\n      const token = req.header('X-Shared-Token');\n      if (!token || token !== WRITE_TOKEN) {\n        return res.status(401).json({ error: 'Unauthorized: invalid or missing shared token' });\n      }\n      return next();\n    }\n    ```\n  - Apply only to **write routes**.\n  - Store token only in env; never log it.\n\n- **Per-token/IP rate limiting**\n  - Add env toggle: `RATE_LIMIT_ENABLED` (default `false`) and settings like `RATE_LIMIT_WINDOW_MS`, `RATE_LIMIT_MAX_REQUESTS`.\n  - Implement an in-memory sliding window or fixed window counter suitable for single-node deployments (direct path, no distributed infra).\n  - Keying strategy:\n    - Prefer token when present: `key = 'token:' + token`.\n    - Fallback to IP: `key = 'ip:' + req.ip`.\n  - Pseudo-code:\n    ```ts\n    interface RateEntry { count: number; resetAt: number; }\n    const rateStore = new Map<string, RateEntry>();\n\n    function rateLimitMiddleware(req, res, next) {\n      if (process.env.RATE_LIMIT_ENABLED !== 'true') return next();\n\n      const windowMs = Number(process.env.RATE_LIMIT_WINDOW_MS ?? 60000);\n      const maxReq = Number(process.env.RATE_LIMIT_MAX_REQUESTS ?? 60);\n\n      const token = req.header('X-Shared-Token');\n      const key = token ? `t:${token}` : `ip:${req.ip}`;\n      const now = Date.now();\n      const entry = rateStore.get(key) ?? { count: 0, resetAt: now + windowMs };\n\n      if (now > entry.resetAt) {\n        entry.count = 0;\n        entry.resetAt = now + windowMs;\n      }\n\n      entry.count += 1;\n      rateStore.set(key, entry);\n\n      if (entry.count > maxReq) {\n        res.setHeader('Retry-After', Math.ceil((entry.resetAt - now)/1000));\n        return res.status(429).json({ error: 'Rate limit exceeded' });\n      }\n\n      res.setHeader('X-RateLimit-Remaining', Math.max(0, maxReq - entry.count));\n      return next();\n    }\n    ```\n  - Middleware order for write routes: `sharedSecret -> rateLimit -> handler`.\n\n- **Defaults for local use**\n  - Document that `SHARED_WRITE_TOKEN` unset and `RATE_LIMIT_ENABLED=false` yields behavior equivalent to current system (no auth, no limiting).\n  - Provide safe defaults (e.g., `60 req/min`) when rate limiting is enabled without explicit settings.\n\n- **Security considerations**\n  - Encourage HTTPS termination in deployment docs (Task 6), even though implementation remains protocol-agnostic.\n  - Consider trusting `X-Forwarded-For` only when behind known proxies; keep IP detection simple in code but document caveats.\n\n- **Extensibility**\n  - Abstract rate limit store so a Redis-based implementation can be plugged in later without changing middleware signature.\n",
        "testStrategy": "- **Unit tests**\n  - For `sharedSecretMiddleware`:\n    - With no `SHARED_WRITE_TOKEN`, verify all requests pass through.\n    - With token set, verify:\n      - Missing header → 401.\n      - Wrong token → 401.\n      - Correct token → next() called.\n  - For `rateLimitMiddleware` with in-memory store:\n    - With `RATE_LIMIT_ENABLED` not `true`, verify pass-through.\n    - With enabled, simulate more than `RATE_LIMIT_MAX_REQUESTS` within a window and expect 429 and `Retry-After` header.\n    - Simulate crossing the window boundary and ensure counters reset.\n\n- **Integration tests**\n  - Attach both middlewares to write endpoints and:\n    - Hit endpoint without token when required → 401.\n    - Hit with correct token repeatedly until hitting configured limit → last one returns 429.\n    - Verify `X-RateLimit-Remaining` decrements as expected.\n  - Confirm that read-only endpoints remain accessible without token and are not rate limited.\n\n- **Security tests**\n  - Ensure logs do not include the shared secret by searching log output for token values in test runs.\n  - Fuzz header parsing with long or malformed token strings to ensure no crashes or excessive memory usage.",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T16:49:37.969Z"
      },
      {
        "id": "4",
        "title": "Resilience, retries, structured logging, SSE error propagation, and health/config endpoint",
        "description": "Improve upstream resilience with retries and tuned timeouts, implement structured logging, propagate provider failures to the frontend via SSE error events, and add an optional health/config endpoint that exposes enabled providers without secrets.",
        "details": "Implementation outline:\n\n- **Retries with backoff for upstream LLM calls**\n  - In the provider abstraction (Task 1), wrap outbound HTTP calls with retry logic for **idempotent** operations (LLM generation is not strictly idempotent, but for robustness limited retries are acceptable on network errors/timeouts).\n  - Use exponential backoff with jitter; e.g., `p-retry@6` or a custom implementation:\n    ```ts\n    async function withRetries<T>(fn: () => Promise<T>, opts: { retries: number; baseDelayMs: number }): Promise<T> {\n      let attempt = 0;\n      while (true) {\n        try {\n          return await fn();\n        } catch (err) {\n          attempt++;\n          if (attempt > opts.retries) throw err;\n          const delay = opts.baseDelayMs * 2 ** (attempt - 1) * (0.5 + Math.random());\n          await new Promise(r => setTimeout(r, delay));\n        }\n      }\n    }\n    ```\n  - Apply a small number of retries (e.g., 2–3) on transient errors (ECONNRESET, ETIMEDOUT, 5xx). Do **not** retry on 4xx.\n\n- **Shorter timeouts for title generation**\n  - Title and auxiliary prompts should use more aggressive timeouts (e.g., 3–5s) versus main conversation turns (e.g., 30–60s).\n  - In each provider client, accept a `timeoutMs` override. For title generation functions, call providers with the shorter timeout; on timeout, surface a non-fatal error and skip title update rather than failing the whole conversation.\n\n- **Structured logging**\n  - Adopt a structured logger such as `pino@9` or `winston@3`; prefer `pino` for performance.\n  - Standardize log fields:\n    - `level` (`info`, `warn`, `error`)\n    - `timestamp`\n    - `requestId` (attach from middleware using `x-request-id` or generated UUID)\n    - `conversationId`, `provider`, `model`, `role`\n    - `errorCode`, `statusCode` for failures.\n  - Example usage:\n    ```ts\n    logger.info({ provider, model, role }, 'LLM request start');\n    logger.error({ provider, model, role, err }, 'LLM request failed');\n    ```\n  - Ensure secrets (API keys, shared tokens) are filtered via logger serializers or redaction settings.\n\n- **SSE error propagation**\n  - For endpoints that stream via **Server-Sent Events (SSE)**, define a consistent error event protocol:\n    - Event name: `error`\n    - Data payload JSON, e.g.:\n      ```json\n      {\n        \"type\": \"provider_error\",\n        \"provider\": \"openai\",\n        \"message\": \"Upstream timeout\",\n        \"retryable\": true\n      }\n      ```\n  - Pseudo-code for sending errors:\n    ```ts\n    function sseSend(res, event, data) {\n      res.write(`event: ${event}\\n`);\n      res.write(`data: ${JSON.stringify(data)}\\n\\n`);\n    }\n\n    function handleLLMError(res, err, ctx) {\n      const payload = {\n        type: 'provider_error',\n        provider: ctx.provider,\n        message: err.message,\n        retryable: isRetryable(err),\n      };\n      sseSend(res, 'error', payload);\n    }\n    ```\n  - Use this for:\n    - Provider unavailability / missing config (from Task 1 fail-fast checks when invoked at runtime).\n    - Network timeouts and 5xx.\n    - Research role unavailable (explicit error type like `research_unavailable`).\n\n- **Health/config endpoint**\n  - Add optional endpoint, e.g. `GET /health` or `GET /config/providers`.\n  - Response content should **not** leak secrets, only high-level configuration like enabled providers and which roles are active:\n    ```json\n    {\n      \"status\": \"ok\",\n      \"providers\": {\n        \"openai\": {\"enabled\": true},\n        \"anthropic\": {\"enabled\": false},\n        \"gemini\": {\"enabled\": true},\n        \"perplexity\": {\"enabled\": false},\n        \"openrouter\": {\"enabled\": true}\n      },\n      \"roles\": {\n        \"council\": [\"openai:gpt-4.1\", \"openrouter:gpt-4.1\"],\n        \"chairman\": \"anthropic:claude-3-5-sonnet\",\n        \"research\": null\n      }\n    }\n    ```\n  - Add env flag `EXPOSE_HEALTH_ENDPOINT=true|false` to guard exposure in sensitive environments.\n\n- **Observability hooks**\n  - Optionally emit basic metrics counters (e.g., Prometheus-style) for requests per provider, failures, and latency percentiles, but keep implementation minimal: e.g., capture timestamps and compute simple averages for logging.\n\n- **Error taxonomy**\n  - Define internal error classes (e.g., `ProviderConfigError`, `ProviderTimeoutError`, `ProviderRateLimitError`) and map them uniformly to SSE error payloads and HTTP error responses.\n",
        "testStrategy": "- **Unit tests**\n  - Test `withRetries` with a function that fails N times then succeeds; verify expected number of attempts and delay behavior (use fake timers).\n  - Ensure non-retryable errors (e.g., 4xx) are not retried.\n  - Test provider clients honor `timeoutMs` by aborting requests and throwing a timeout error.\n  - Test logger integration: ensure log entries include `provider`, `model`, `role`, and no secrets.\n  - Test SSE helper `sseSend` formats messages correctly for different event names.\n\n- **Integration tests**\n  - Simulate upstream LLM timeouts and 5xx using an HTTP mock server; confirm retries happen, then an SSE `error` event is emitted and connection is cleanly closed.\n  - Force a misconfiguration (missing API key for referenced provider) and verify that an SSE `error` with type `provider_error` or `config_error` is sent when that role is invoked.\n  - Call health/config endpoint with various env configurations and assert secrets are not present while enabled/disabled states are accurate.\n\n- **Load & resilience tests**\n  - Under moderate load, confirm retry behavior does not overwhelm upstream services (limit retries) and that logging remains performant (e.g., using `pino` in async mode).\n  - Verify that title generation timeouts do not impact the main response path: users still receive conversation replies even if titles fail.",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T16:49:37.973Z"
      },
      {
        "id": "5",
        "title": "Frontend and backend data safety messaging",
        "description": "Add clear data safety messaging explaining that data/conversations/ stores unencrypted local JSON, ensure data/ remains git-ignored, and implement a frontend banner or notice linking to storage warnings and configuration notes.",
        "details": "Implementation outline:\n\n- **Backend messaging source**\n  - Provide a low-friction way to surface data safety messaging to the frontend, e.g. via a small config endpoint or embedding in an existing bootstrap/config API:\n    ```json\n    {\n      \"storage\": {\n        \"path\": \"data/conversations/\",\n        \"encrypted\": false,\n        \"description\": \"Conversations are stored as unencrypted JSON files on local disk.\",\n        \"docsPath\": \"#storage-safety\"\n      }\n    }\n    ```\n  - Hard-code that conversations are **unencrypted local JSON**; do not attempt automatic disk encryption.\n\n- **Ensure .gitignore covers data/**\n  - Verify `.gitignore` includes at least:\n    ```\n    data/\n    data/*\n    ```\n  - If repo uses language-specific templates, append `data/` and ensure `data/conversations/` is not accidentally committed.\n\n- **Frontend banner/notice**\n  - Add a non-intrusive but visible banner to the main app view (e.g., top of page or settings panel):\n    - Text along the lines of: \"Conversations are stored as **unencrypted JSON files** under `data/conversations/` on this machine. Do not use for sensitive data.\"\n    - Include a \"Learn more\" link that opens a modal or navigates to a help page/section describing storage behavior and configuration (e.g., ability to change paths, retention, etc., if supported).\n  - Allow banner dismissal per user (localStorage flag) but make it rediscoverable via a help icon or settings.\n\n- **UI implementation sketch** (React example):\n    ```tsx\n    const STORAGE_DISMISSED_KEY = 'storageWarningDismissed';\n\n    function StorageWarningBanner() {\n      const [dismissed, setDismissed] = useState(() => localStorage.getItem(STORAGE_DISMISSED_KEY) === '1');\n      if (dismissed) return null;\n      return (\n        <div className=\"storage-banner\">\n          <p>\n            Conversations are stored as <strong>unencrypted JSON files</strong> under <code>data/conversations/</code> on this machine.\n            Avoid storing sensitive or personal data.\n          </p>\n          <button onClick={() => { localStorage.setItem(STORAGE_DISMISSED_KEY, '1'); setDismissed(true); }}>Dismiss</button>\n          <button onClick={openStorageDetails}>Learn more</button>\n        </div>\n      );\n    }\n    ```\n\n- **Storage warning details**\n  - In the detailed UI (modal or settings page), explain:\n    - Storage location: `data/conversations/`.\n    - Data format: unencrypted JSON.\n    - That deleting the folder deletes conversation history.\n    - That encryption, off-disk storage, or rotation are out of scope unless separately configured.\n\n- **Configuration notes linkage**\n  - The \"Learn more\" component should reference documentation about:\n    - Provider configuration.\n    - Auth/rate limiting.\n    - SSE error behaviors.\n  - Implementation can navigate to an in-app \"Help / Docs\" route or open an embedded markdown page.\n",
        "testStrategy": "- **Unit tests (frontend)**\n  - Test `StorageWarningBanner` renders when `storageWarningDismissed` is not set and hides after clicking Dismiss.\n  - Verify that the \"Learn more\" button triggers navigation or modal opening as expected.\n\n- **Unit tests (backend)**\n  - If a storage/config endpoint is exposed, test that it reports `encrypted: false` and the correct storage path.\n\n- **Integration tests**\n  - End-to-end UI test (e.g., Playwright/Cypress):\n    - First load: banner is visible, with correct text mentioning `data/conversations/` and unencrypted JSON.\n    - After dismissing and reloading, banner is hidden.\n  - Verify `.gitignore` is present in the repo and contains `data/` entries; as a CI safeguard, optionally run a script that fails if any file under `data/` is tracked by Git in test runs.\n\n- **Accessibility and UX checks**\n  - Confirm banner has sufficient contrast, is screen-reader friendly, and is dismissible via keyboard.",
        "priority": "medium",
        "dependencies": [
          "2",
          "4"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T16:49:37.977Z"
      },
      {
        "id": "6",
        "title": "Documentation and configuration updates for providers, auth, rate limits, SSE errors, and storage",
        "description": "Update .env.example, README, and UI/docs to describe provider keys and notation, shared secret auth, rate limit toggles, SSE error handling expectations, and storage safety behavior.",
        "status": "done",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5"
        ],
        "priority": "medium",
        "details": "Implementation outline:\n\n**COMPLETED WORK:**\n- ✅ Comprehensive README.md rewrite completed (commit 4a40e2d)\n- ✅ .env.example already updated throughout Tasks 1-5 with all necessary configuration variables\n\n**REMAINING WORK:**\n\n- **UI documentation integration**\n  - Ensure frontend banner/notice from Task 5 properly links to the new \"Data Storage & Safety\" section in README\n  - Verify all documentation links in UI components point to correct README sections\n  - Test that storage warning modal/banner displays accurate information matching README content\n\n- **Documentation validation & consistency**\n  - Cross-check that all environment variable names in README exactly match those used in code\n  - Verify API endpoint documentation matches actual implementation\n  - Ensure SSE error event examples in README reflect actual runtime payloads\n\n- **Developer experience improvements**\n  - Add any missing inline code comments referencing README sections for complex configurations\n  - Ensure error messages in code reference appropriate documentation sections where helpful\n\n- **Optional automated consistency checks**\n  - Consider implementing a lightweight CI check to ensure `.env.example` and README remain in sync with required env vars\n  - Script could scan for `process.env.*` usages and verify they are documented in `.env.example`\n\nThe major documentation work is complete - this task now focuses on integration, validation, and maintaining consistency between documentation and implementation.",
        "testStrategy": "- **Documentation review**\n  - Peer review README and `.env.example` for accuracy and clarity, cross-checking with actual code (env variable names, endpoints, header names).\n\n- **Integration testing**\n  - Verify UI banner/notice links correctly navigate to README sections\n  - Test that storage warning content matches between frontend components and README\n  - Follow README setup steps on a fresh clone to ensure complete accuracy\n\n- **Runtime validation**\n  - Capture actual SSE error payloads and compare with documented examples\n  - Verify health/config endpoint responses match documented format\n  - Test all documented configuration scenarios work as described\n\n- **Automated checks (optional but recommended)**\n  - Implement a small test script that:\n    - Parses the codebase for `process.env.*` usages.\n    - Verifies each appears in `.env.example`.\n  - Run this script as part of CI, failing if discrepancies are found.\n\n- **UX validation**\n  - Confirm that links from the UI banner or settings to the storage/config documentation sections are correct and not broken.",
        "subtasks": [
          {
            "id": 1,
            "title": "Validate UI documentation integration",
            "description": "Ensure frontend components properly link to and reference the completed README documentation sections.",
            "dependencies": [],
            "details": "Test that storage warning banner/modal from Task 5 correctly links to the \"Data Storage & Safety\" section in README. Verify all documentation links in UI components point to correct sections and that displayed information matches README content.",
            "status": "pending",
            "testStrategy": "Manual testing of UI links and content consistency with README",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Cross-validate documentation accuracy",
            "description": "Verify that all documented configuration, endpoints, and examples exactly match the actual implementation.",
            "dependencies": [],
            "details": "Cross-check environment variable names, API endpoint paths, header names, SSE event formats, and configuration examples in README against actual code implementation. Capture real runtime payloads to validate documented examples.",
            "status": "pending",
            "testStrategy": "Code review and runtime payload comparison",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement optional consistency automation",
            "description": "Create automated checks to maintain documentation-code consistency over time.",
            "dependencies": [],
            "details": "Develop a lightweight script that scans codebase for process.env.* usages and verifies each appears in .env.example with appropriate documentation. Consider adding this as a CI check to prevent documentation drift.",
            "status": "pending",
            "testStrategy": "Test script with known env vars and verify CI integration",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T16:49:16.744Z"
      },
      {
        "id": "7",
        "title": "Conversation context threading in council stages",
        "description": "Enable the council pipeline to reuse previous user questions and chairman replies so follow-up queries receive context-aware answers when sent via the backend messaging APIs.",
        "details": "• Extend `run_full_council` to accept `conversation_history: List[Dict[str, Any]]`, and build a context message list from prior user prompts and Stage 3 responses according to a new `CONVERSATION_CONTEXT_STRATEGY` env option (default to “chairman-and-users”).\n• Teach `stage1_collect_responses` and `stage3_synthesize_final` to accept this message history and prepend it when calling LLM providers, ensuring Stage 1 and Stage 3 both see the same contextual transcript.\n• Update `backend/main.py`’s `send_message` and `send_message_stream` to read prior exchanges from storage, honor the strategy setting, and pass the assembled list into `run_full_council` so SSE and non-stream calls share logic.\n• Document supported strategies (e.g., `users_and_chairman`, `all_messages`) and ensure the strategy can be toggled without restart via refreshed env/config handling.\n• Guard costs by limiting history length and filtering only the roles dictated by the chosen strategy; add inline comments describing any truncation heuristics.",
        "testStrategy": "• Unit tests for a helper that builds the context list, covering both strategy modes and ensuring chairman responses are captured after Stage 3 emits them.\n• Integration tests for `send_message`/`send_message_stream` mocking stored histories to verify `run_full_council` receives the correct message arrays.\n• Functional test (or manual run) sending a follow-up query to confirm Stage 1 and Stage 3 outputs reference earlier conversation state and that strategy toggles alter the prompt content.",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-10T08:01:26.855Z"
      },
      {
        "id": "8",
        "title": "Dynamic model configuration API and frontend selector",
        "description": "Implement server endpoints, registry metadata, and frontend UI to let users inspect available models, choose presets, and store per-conversation model overrides without restarts.",
        "details": "1. Extend the backend model registry to annotate each model with display name, provider, estimated cost tier (low/medium/high), and speed tier; define preset bundles (Fast/Balanced/Comprehensive) that map to existing council/chairman/research roles.\n2. Add REST endpoints: `GET /api/config/models` returning all models grouped by provider with metadata; `GET /api/config/providers` listing providers and whether they are enabled; `GET /api/config/current` exposing the currently active council/chairman/research models and preset if applicable; update `POST /api/conversations` to accept an optional `models` payload (preset or explicit selection) and persist it in conversation metadata for downstream attribution.\n3. Update the council execution pipeline to read the stored conversation model overrides (falling back to global defaults) before invoking providers so chair/council/research stages honor per-conversation choices.\n4. Build a frontend settings/conversation-creation UI that fetches the config endpoints, shows models grouped by provider, displays cost/speed badges, offers preset shortcuts, and allows advanced users to customize each stage’s model; ensure selections are submitted with the conversation creation request and surfaced in conversation details for later reference.",
        "testStrategy": "• Backend unit tests for the model registry helpers (metadata enrichment, preset resolution) and the new config endpoints, verifying schema and access control.\n• Integration tests for `POST /api/conversations` showing custom model payloads are validated, stored, and later used when invoking the council pipeline.\n• Frontend component tests for the selector UI to confirm grouping, badge rendering, preset selection, and form submission payloads.\n• End-to-end test (or manual QA script) that creates a conversation with a preset override, inspects stored metadata, and confirms subsequent responses cite the chosen models.",
        "status": "done",
        "dependencies": [
          "4",
          "7"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Augment backend model registry with metadata and presets",
            "description": "Extend the registry schema so every model includes display name, provider, cost tier, and speed tier plus define Fast/Balanced/Comprehensive preset bundles aligned to council/chairman/research roles.",
            "dependencies": [],
            "details": "Identify the registry module, add metadata fields to the model descriptors, document allowed tier enums, and implement helper functions to resolve presets into concrete per-role selections.",
            "status": "done",
            "testStrategy": "Unit tests for metadata helpers and preset resolution logic.",
            "parentId": "undefined",
            "updatedAt": "2025-12-10T08:16:10.997Z"
          },
          {
            "id": 2,
            "title": "Expose model and provider listing endpoints",
            "description": "Create GET /api/config/models and GET /api/config/providers endpoints that pull from the enriched registry, grouping models by provider and indicating provider enablement flags.",
            "dependencies": [
              1
            ],
            "details": "Implement serializer utilities that structure grouped responses, ensure provider availability is derived from configuration, and enforce existing auth/access controls on these endpoints.",
            "status": "done",
            "testStrategy": "API tests verifying JSON schema, grouping correctness, and disabled provider behavior.",
            "parentId": "undefined",
            "updatedAt": "2025-12-10T09:44:20.814Z"
          },
          {
            "id": 3,
            "title": "Report current configuration and accept overrides on conversation creation",
            "description": "Add GET /api/config/current to expose active council/chairman/research models and active preset, and update POST /api/conversations to accept optional preset or explicit models payload, validate it, and persist to conversation metadata.",
            "dependencies": [
              1,
              2
            ],
            "details": "Design a validation schema for incoming overrides, reuse preset helpers, store selections alongside the conversation record, and ensure audit logging/error responses mirror existing style.",
            "status": "done",
            "testStrategy": "API tests covering successful preset use, explicit per-role selection, invalid payload rejection, and persistence inspection.",
            "parentId": "undefined",
            "updatedAt": "2025-12-10T11:28:18.124Z"
          },
          {
            "id": 4,
            "title": "Honor stored model overrides in council execution pipeline",
            "description": "Update the council/chair/research invocation flow to read any conversation-level overrides before calling providers, falling back to global defaults when none exist.",
            "dependencies": [
              3
            ],
            "details": "Modify the pipeline entry points to fetch metadata, propagate chosen models into each stage, and add safeguards when requested models are unavailable at runtime.",
            "status": "done",
            "testStrategy": "Integration tests stubbing the pipeline to confirm overrides are applied and defaults remain when metadata is absent.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T09:46:18.029Z"
          },
          {
            "id": 5,
            "title": "Implement frontend config data hooks and state",
            "description": "Create frontend services/hooks to fetch config endpoints, normalize provider-grouped models, expose presets, and maintain per-stage selections during conversation setup.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add API client methods, caching/refresh logic, and a context or store representing current preset, cost/speed labels, and advanced per-stage selections ready for UI consumption.",
            "status": "done",
            "testStrategy": "Frontend unit tests ensuring hooks fetch once, map data correctly, and update state when users toggle presets or custom models.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T09:53:47.339Z"
          },
          {
            "id": 6,
            "title": "Build conversation creation UI for model selection and display overrides",
            "description": "Update the conversation creation flow/UI to show provider-grouped models with cost/speed badges, offer preset shortcuts, allow advanced per-stage customization, submit selections with POST /api/conversations, and surface chosen models in conversation detail views.",
            "dependencies": [
              5
            ],
            "details": "Design responsive components, wire form submission to include the models payload, add visual confirmation badges in conversation headers/details, and ensure fallback display when defaults are used.",
            "status": "done",
            "testStrategy": "Frontend integration tests verifying UI renders model metadata, forms submit correct payloads, and conversation detail pages show persisted selections.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T11:55:20.244Z"
          }
        ],
        "updatedAt": "2025-12-11T11:55:20.244Z"
      },
      {
        "id": "9",
        "title": "Council size validation and dynamic composition support",
        "description": "Add backend validation, dynamic council overrides, new configuration endpoints, and UI enhancements to display council participation details.",
        "details": "• Extend the council configuration module to validate COUNCIL_MODELS at startup: enforce min 2 and max 7 entries, fail-fast on invalid counts, and emit warnings when all entries share the same provider.\n• Introduce a per-request override path that lets API consumers pass `council_models` when posting a conversation message; ensure the override list is validated against the registered providers/models and fallback to defaults when absent.\n• Create `GET /api/config/council` returning the active council composition with provider and capability metadata, plus `POST /api/config/council/validate` that runs the same validation logic on arbitrary model arrays without mutating state.\n• Propagate council membership data to the frontend so the header/settings view can show current members and per-model status in Stage 1 results, and annotate responses with the models that participated.\n• Document any new env vars or payload fields alongside error responses for invalid overrides, reusing the structures introduced in Task 8’s registry work.",
        "testStrategy": "• Backend unit tests for the validation helper covering boundary counts, provider diversity warnings, and error messages triggered on invalid configurations.\n• API tests for `POST /api/conversations/{id}/message` verifying accepted overrides, rejected unknown models, and fallback behavior when limits are exceeded.\n• Endpoint tests for `GET /api/config/council` and `POST /api/config/council/validate` asserting response schemas and error handling.\n• Frontend component tests that the council display renders the provided members, shows status indicators, and marks which models contributed to each response.",
        "status": "done",
        "dependencies": [
          "8"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-11T12:37:30.889Z"
      },
      {
        "id": "10",
        "title": "Implement MCP server for council access",
        "description": "Build an MCP server package that exposes the council deliberation features as MCP tools and document how to integrate it via .mcp.json.",
        "details": "• Create an `mcp/` package with `__init__.py` and `server.py` leveraging the MCP SDK; support both stdio and SSE transports so the server can run in different host environments.\n• Initialize the server with the existing backend provider/model registry (Task 8) so council/chairman/research model presets and configuration overrides are reused; load backend config to honor shared provider metadata and council override validation rules.\n• Implement MCP tools:\n  – `council_query` (submit prompt, return 3-stage transcript) and `council_query_with_models` (accept explicit model overrides validated against registry metadata).\n  – `get_conversation`, `list_conversations` (with pagination controls), and `continue_conversation` respecting backend conversation storage semantics.\n• Define schemas for each tool: precise input parameters (types, descriptions, validations such as required conversation IDs, optional pagination cursors, override lists) and documented output envelopes (success payload, error_type/error_message fields) plus standardized error handling.\n• Ensure conversation lifecycle wiring: reuse backend council pipeline functions so history-aware queries continue to function after Task 7, and marshal conversation IDs/results for MCP clients.\n• Add repository-root `.mcp.json` example configuring the server command, env vars (API keys, provider settings), and transport mode hints for Claude Code/Cursor integrations.\n• Document setup in README/docs: installation steps, running MCP server (stdio vs SSE), configuration expectations, and how to use the provided MCP toolset from client IDEs; note dependency on existing backend config and authentication controls.",
        "testStrategy": "• Unit tests for MCP tool handlers covering schema validation (missing IDs, invalid override models, pagination bounds) and error serialization; mock backend registry/conversation services to ensure correct calls.\n• Integration test that starts the MCP server in stdio mode against the backend services and exercises each tool end-to-end, verifying council queries return 3-stage transcripts, overrides are honored, and conversation continuation works.\n• Manual verification of `.mcp.json` example by launching an IDE client (e.g., Claude Code) pointing to the sample config and executing a council query.\n• Documentation check: follow README/MCP instructions from a clean environment to confirm setup succeeds without missing steps.",
        "status": "done",
        "dependencies": [
          "8",
          "7"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-11T12:23:02.675Z"
      },
      {
        "id": "11",
        "title": "Dockerize backend and frontend services",
        "description": "Add containerization support so the backend and frontend can be built, composed, and deployed via Docker with production defaults.",
        "details": "• Backend Dockerfile: base on python:3.11-slim, install curl && uv, copy backend source and pyproject/config files, run `uv sync --frozen` (or equivalent) in production mode, configure non-root user, set `PORT=8001`, expose 8001, define `CMD` using uvicorn/gunicorn with graceful shutdown signals, and ensure health endpoint readiness.\n• Frontend Dockerfile: multi-stage (node:lts for build, nginx:alpine for serve). Stage 1 installs dependencies via corepack/pnpm or npm, runs `npm run build` with production env vars. Stage 2 copies build artifacts to `/usr/share/nginx/html`, copies custom nginx.conf enabling SPA routing (try_files fallback) and gzip, exposes 80.\n• docker-compose.yml: define backend and frontend services referencing Dockerfiles, connect via shared network, inject `.env` variables, mount `data/conversations/` volume, configure healthchecks (`curl`/`wget` for backend, `nginx -t` or HTTP for frontend), and optionally define redis service with persistent volume for rate limiting.\n• Add `.dockerignore` files for backend/frontend (exclude node_modules, venvs, tests, docs, build artifacts) plus nginx.conf checked into repo.\n• Documentation: extend README with Docker deployment instructions (build/run commands, env requirements, volume mounts, redis optional service) and environment variable table; note persistence expectations and shutdown behavior.",
        "testStrategy": "• Build images locally: `docker build` for backend and frontend to ensure Dockerfiles succeed and images run with expected entrypoints.\n• `docker-compose up --build` from clean environment verifies both services start, share network, load .env, mount volumes, and healthchecks pass; confirm frontend serves SPA and proxies to backend.\n• Confirm data persistence by writing a conversation via the app, stopping containers, restarting compose, and verifying data remains in `data/conversations/`.\n• Validate README instructions by following documented steps end-to-end on a fresh clone.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-10T07:28:37.266Z"
      },
      {
        "id": "12",
        "title": "Implement webhook notifications for council events",
        "description": "Add configurable signed webhook delivery so external systems can receive conversation lifecycle events without interrupting the main response flow.",
        "details": "1. Configuration: extend backend settings loader to read `WEBHOOK_URL` and `WEBHOOK_SECRET`, allow per-conversation override stored in metadata, and expose the resolved webhook configuration (global + per-conversation capability flags) on `GET /api/config` alongside existing Task 8 model data. 2. Event emission: instrument conversation creation, stage completion, final response, and error paths to emit structured payloads containing event name, ISO-8601 timestamp, conversation UUID, stage/result data, and calculated `signature = hex(hmac_sha256(secret, payload_json))`. 3. Delivery worker: send events asynchronously via a non-blocking job (e.g., background task or queue) that retries on non-2xx responses or network failures with exponential backoff delays of 1s/2s/4s, logging each attempt and final success/failure without disrupting the user-facing request. 4. Reliability features: support disabling webhooks when no URL is configured, ensure per-conversation URLs override the global one when present, and redact secrets from logs. 5. Documentation: update README with setup instructions, environment variables, payload schema, available events, retry policy, and troubleshooting notes.",
        "testStrategy": "• Unit tests for the HMAC signer and payload builder verifying deterministic signatures and correct inclusion of event metadata. • Backend tests simulating conversation lifecycle to assert that each event type enqueues the proper payload and respects per-conversation webhook URLs and fallbacks. • Integration test using a mock HTTP server that forces failures to confirm the 3-attempt exponential backoff schedule and that errors do not block API responses but are logged. • Documentation check by rendering README section to ensure webhook configuration and payload examples are accurate.",
        "status": "done",
        "dependencies": [
          "8"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-11T12:44:01.974Z"
      },
      {
        "id": "13",
        "title": "Implement conversation export/import flows",
        "description": "Add backend endpoints and frontend features so users can export individual or bundled conversations (JSON/Markdown/ZIP) and import either conversation archives or new documents for review.",
        "details": "Backend:\n• Extend conversation service with serializers that gather full transcript, model metadata, timestamps, and aggregate rankings, and emit them either as structured JSON or Markdown rendered per the required template (including council roster, chairman, stage sections, rankings table).\n• Add GET /api/conversations/{id}/export handling format=json|markdown (default json) using UUID validation and secure file reads from data/conversations; support ?collection=true to bundle multiple conversation files into a ZIP.\n• Create collection export utility that streams ZIP archives without loading all files in memory and records manifest metadata.\n• Implement POST /api/conversations/import accepting multipart/form-data with JSON exports or ZIP collections; validate schema, ensure conversation IDs remain UUID v4 (generate new ones if missing/colliding), and persist via existing storage layer.\n• Implement POST /api/conversations/review to accept file uploads (PRD/docs), store them under dedicated folder, and enqueue a council conversation referencing the uploaded document for Stage 1 inputs.\n• Reuse shared-secret middleware and rate limiting from Task 3 automatically for new write endpoints; ensure file uploads respect size caps.\nFrontend:\n• In conversation view, add Export button with dropdown for JSON/Markdown/ZIP and call the new endpoint, triggering browser download.\n• In conversation list page, add Import control allowing drag/drop or file picker for single JSON/Markdown/ZIP files and display validation errors; trigger import API and refresh list.\n• Add \"Review Document\" action surfaced near conversation list header that opens modal to upload a document and select council models/metadata before invoking /review.\nDocumentation:\n• Update help/FAQ pages describing export formats and import workflows, emphasizing storage safety already outlined in Task 5.\nOperational considerations:\n• Backups of import/export directories belong under data/ and stay gitignored; log events for auditing.\n",
        "testStrategy": "Backend tests:\n• Unit tests for JSON and Markdown serializer helpers verifying metadata inclusion and markdown structure (heading order, ranking tables) plus ZIP collection builder ensuring manifest accuracy.\n• API tests for GET /api/conversations/{id}/export covering both formats, invalid IDs, collection exports, and permission enforcement; verify markdown body matches expected template snapshot.\n• Integration tests uploading sample JSON and ZIP files via POST /api/conversations/import confirming conversations persist with UUIDs, duplicates are handled, and malformed payloads return 400.\n• Tests for POST /api/conversations/review verifying multipart uploads succeed, documents stored securely, and council jobs are queued with references.\nFrontend tests:\n• Component tests for export dropdown wiring ensuring correct endpoint invocation and download handling.\n• UI tests for import modal validating client-side schema checks and success/error states; ensure Review Document flow submits multipart data.\nManual verification:\n• Export an existing conversation in both formats, inspect Markdown output, import it in a clean environment, and confirm the restored conversation renders correctly.\n",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Design export/import data schema and validations",
            "description": "Document the JSON, Markdown, and ZIP manifest schemas plus validation rules for conversation exports/imports.",
            "dependencies": [],
            "details": "Define required fields (transcript, metadata, rankings, timestamps) and specify how UUID collisions/replacements work; capture Markdown template sections and collection manifest structure for both backend and frontend consumers.",
            "status": "done",
            "testStrategy": null,
            "updatedAt": "2025-12-11T13:01:50.262Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement conversation serialization helpers",
            "description": "Extend backend conversation service with JSON/Markdown serializers and reusable ZIP streaming utility.",
            "dependencies": [
              1
            ],
            "details": "Create serializers that gather full transcript, metadata, and rankings, rendering Markdown via the approved template and enabling stream-based ZIP bundling with manifest metadata without loading all files into memory.",
            "status": "done",
            "testStrategy": "Unit tests covering JSON/Markdown output structure and ZIP manifest integrity.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:01:50.267Z"
          },
          {
            "id": 3,
            "title": "Build export API endpoints",
            "description": "Add GET /api/conversations/{id}/export supporting JSON/Markdown and optional collection ZIP bundling.",
            "dependencies": [
              2
            ],
            "details": "Validate UUIDs, secure file reads from data/conversations, plug serializers based on format param, and support ?collection=true to bundle multiple conversation files into a streamed ZIP response.",
            "status": "done",
            "testStrategy": "API tests for formats, invalid IDs, collection downloads, and security checks.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:01:50.273Z"
          },
          {
            "id": 4,
            "title": "Implement import and review backend endpoints",
            "description": "Create POST /api/conversations/import and POST /api/conversations/review to handle uploads and queue reviews.",
            "dependencies": [
              2,
              3
            ],
            "details": "Accept multipart uploads, enforce size caps, reuse shared-secret middleware and rate limiting, validate schemas, ensure UUID v4 generation when missing/colliding, persist via storage layer, and enqueue review conversations referencing uploaded documents.",
            "status": "done",
            "testStrategy": "API tests verifying schema validation, UUID handling, size limits, and document review queuing.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:01:50.280Z"
          },
          {
            "id": 5,
            "title": "Integrate audit logging and storage safeguards",
            "description": "Ensure import/export actions log audit events and that backup directories stay under data/ with gitignore compliance.",
            "dependencies": [
              3,
              4
            ],
            "details": "Add structured logs for export/import/review actions and confirm data/ backups remain gitignored per Task 5 safety messaging; document operational considerations in configs.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:01:50.286Z"
          },
          {
            "id": 6,
            "title": "Add frontend export controls in conversation view",
            "description": "Enhance conversation view with an Export dropdown triggering the new backend endpoint downloads.",
            "dependencies": [
              3,
              5
            ],
            "details": "Implement UI button/dropdown for JSON/Markdown/ZIP formats, call export API with selected options, and handle browser download flows with error display.",
            "status": "done",
            "testStrategy": "Frontend component tests verifying dropdown behavior and API interactions.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:08:32.694Z"
          },
          {
            "id": 7,
            "title": "Add frontend import workflow on conversation list",
            "description": "Provide drag/drop and picker-based import controls with validation errors and list refresh.",
            "dependencies": [
              4,
              6
            ],
            "details": "Build UI to accept JSON/Markdown/ZIP files, show schema validation feedback, submit to import API, handle loading states, and refresh conversation list on success.",
            "status": "done",
            "testStrategy": "Frontend tests simulating uploads, error handling, and list refresh triggers.",
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:08:32.699Z"
          },
          {
            "id": 8,
            "title": "Implement review document upload UI and documentation updates",
            "description": "Surface Review Document action, modal flow, and update help/FAQ on export/import usage.",
            "dependencies": [
              4,
              7
            ],
            "details": "Add list-header action opening modal to upload documents, select council metadata, and call /review endpoint; refresh list after scheduling. Update documentation pages explaining export formats, import workflows, safety messaging, and storage locations.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-11T13:08:32.706Z"
          }
        ],
        "updatedAt": "2025-12-11T13:08:32.706Z"
      },
      {
        "id": "14",
        "title": "Implement Redis-backed distributed rate limiting",
        "description": "Introduce an optional Redis-powered rate limiting backend that replaces the current single-instance memory store while automatically falling back to the existing in-memory limiter if Redis is unavailable.",
        "details": "1. Add the async redis-py client (e.g., `redis[hiredis]`) and wire a configurable Redis connection that reads `REDIS_URL`; load it lazily so the service starts even when Redis is absent.\n2. Introduce `RATE_LIMIT_BACKEND` configuration with allowed values `memory` (default) and `redis`; extend the settings loader to validate these options and expose resolved backend info on the existing config surface so the UI or ops tooling can detect which backend is active.\n3. Define an abstract `RateLimiter` contract with `check_limit` returning (allowed, retry-after, remaining) metadata and `get_remaining` for UI introspection; refactor the existing in-memory limiter into `MemoryRateLimiter` implementing the interface.\n4. Implement `RedisRateLimiter` that uses atomic `INCR`/`EXPIRE` (or `INCRBY` with Lua for sliding window) keyed as `rate_limit:{token_or_ip}:{window_start}`; ensure connection pooling, exponential backoff on transient failures, and graceful downgrade to memory when Redis is unreachable.\n5. Update rate limiting middleware to instantiate the proper backend based on configuration, log backend selection, and continue serving traffic using memory when Redis errors occur.\n6. Add Redis health reporting to `/health`: include connectivity status, last error, and fallback state so operators can detect degradation; ensure this endpoint tolerates Redis outages without failing overall health.\n7. Extend `docker-compose.yml` with an optional Redis service and document environment wiring; update README deployment instructions for enabling Redis, setting env vars, and operating in multi-instance mode.",
        "testStrategy": "• Unit tests for the RateLimiter interface implementations: verify MemoryRateLimiter parity with previous behavior and RedisRateLimiter’s atomic counters (use a Redis test container or fakeredis).\n• Configuration tests ensuring invalid RATE_LIMIT_BACKEND values raise errors, Redis is optional, and fallback to memory occurs when REDIS_URL is missing or Redis is down.\n• Middleware integration tests simulating rapid requests across multiple workers to confirm shared limits when Redis is enabled and local enforcement when memory is selected.\n• `/health` endpoint tests verifying Redis status reporting in healthy, degraded, and fallback scenarios.\n• docker-compose smoke test bringing up backend + Redis to ensure connectivity and env variables are wired correctly.",
        "status": "done",
        "dependencies": [
          "3",
          "11"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2025-12-11T17:33:38.784Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-11T17:33:38.786Z",
      "taskCount": 14,
      "completedCount": 14,
      "tags": [
        "master"
      ]
    }
  }
}