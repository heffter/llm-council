{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Provider abstraction, env-driven configuration, and provider:model parsing",
        "description": "Introduce a unified LLM provider abstraction that supports provider:model notation, env-driven per-role configuration (council + chairman + optional research role), and fail-fast validation of models/keys at startup.",
        "details": "Implementation outline:\n\n- **Tech choices & libraries**\n  - Backend language: follow existing codebase; if Node.js/TypeScript, use `node-fetch@3` or `undici@6` for HTTP.\n  - For config: use `zod@3` or `joi@17` to validate env/config objects.\n  - For type-safe env access: `dotenv@16` + a thin wrapper.\n\n- **Config & notation**\n  - Support model identifiers of form `\"provider:model\"`, e.g. `\"openai:gpt-4.1\"`, `\"anthropic:claude-3-5-sonnet\"`, `\"gemini:gemini-2.0-pro\"`, `\"perplexity:sonar-pro\"`, `\"openrouter:gpt-4.1\"`.\n  - Implement a parser:\n    ```ts\n    type ProviderId = 'openai' | 'anthropic' | 'gemini' | 'perplexity' | 'openrouter';\n\n    interface ParsedModelId {\n      provider: ProviderId;\n      model: string; // provider-native model ID\n    }\n\n    function parseProviderModel(id: string): ParsedModelId {\n      const [provider, model] = id.split(':');\n      if (!provider || !model) throw new Error(`Invalid provider:model string: ${id}`);\n      if (!['openai','anthropic','gemini','perplexity','openrouter'].includes(provider)) {\n        throw new Error(`Unsupported provider: ${provider}`);\n      }\n      return { provider: provider as ProviderId, model };\n    }\n    ```\n\n- **Env-driven per-role config**\n  - Define env vars for council and chairman roles, plus optional research role, e.g.:\n    - `COUNCIL_MODELS=openai:gpt-4.1,anthropic:claude-3-5-sonnet`\n    - `CHAIRMAN_MODEL=anthropic:claude-3-5-sonnet`\n    - `RESEARCH_MODEL=perplexity:sonar-pro` (optional)\n  - Map providers to key env vars:\n    - `OPENAI_API_KEY`\n    - `ANTHROPIC_API_KEY`\n    - `GEMINI_API_KEY`\n    - `PERPLEXITY_API_KEY`\n    - `OPENROUTER_API_KEY`\n  - On startup, parse env, build a `ProviderConfig` registry:\n    ```ts\n    interface ProviderConfig {\n      apiKey: string;\n      baseUrl: string;\n      timeoutMs: number;\n    }\n\n    const providerConfigs: Record<ProviderId, ProviderConfig | undefined> = {\n      openai: process.env.OPENAI_API_KEY ? {\n        apiKey: process.env.OPENAI_API_KEY!,\n        baseUrl: process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1',\n        timeoutMs: 60000,\n      } : undefined,\n      // ... similarly for others\n    };\n    ```\n\n- **Provider abstraction**\n  - Define an interface similar to patterns used in multi-LLM platforms[3]:\n    ```ts\n    interface LLMProviderClient {\n      complete(opts: {\n        model: string;\n        messages: { role: 'system'|'user'|'assistant'; content: string }[];\n        temperature?: number;\n        maxTokens?: number;\n      }): Promise<AsyncIterable<string> | string>; // depending on streaming\n    }\n    ```\n  - Implement concrete clients:\n    - **OpenAI**: use current `chat.completions` API (`gpt-4.1`, `gpt-4.1-mini`, `o3-mini`) with `Authorization: Bearer`.\n    - **Anthropic**: use `/v1/messages` for Claude 3.x (e.g., `claude-3-5-sonnet-latest`), `x-api-key` header.\n    - **Gemini**: use `v1beta/models/{model}:streamGenerateContent` or non-streaming `generateContent`.\n    - **Perplexity**: use `/chat/completions` compatible with OpenAI format where available.\n    - **OpenRouter**: proxy OpenAI-compatible requests to `https://openrouter.ai/api/v1` with `Authorization: Bearer`.\n\n- **Routing by role**\n  - In the council orchestration layer, map logical roles to parsed models:\n    ```ts\n    interface RoleConfig {\n      id: string; // 'chairman' | 'council' | 'research'\n      modelId: string; // provider:model\n    }\n\n    const councilModels = process.env.COUNCIL_MODELS?.split(',') ?? [];\n    const chairmanModel = process.env.CHAIRMAN_MODEL;\n    const researchModel = process.env.RESEARCH_MODEL; // optional\n    ```\n  - Expose a function:\n    ```ts\n    async function callRoleLLM(role: 'council'|'chairman'|'research', payload: PromptPayload) {\n      const modelId = selectModelForRole(role); // e.g., round-robin across council list\n      const { provider, model } = parseProviderModel(modelId);\n      const client = getProviderClient(provider);\n      if (!client) throw new Error(`Provider not configured: ${provider}`);\n      return client.complete({ model, messages: payload.messages });\n    }\n    ```\n\n- **Fail-fast validation**\n  - On application startup:\n    - Parse all configured provider:model IDs.\n    - For each provider referenced by any role, assert corresponding API key env var is present; if not, throw and log explicit message: e.g., `Missing OPENAI_API_KEY for configured model openai:gpt-4.1`.\n    - For optional **research role**: if `RESEARCH_MODEL` set but provider key missing, log a warning and disable research role; otherwise keep research role disabled by default.\n  - Distinguish between **hard errors** (core roles) vs **optional** (research) when exiting.\n\n- **Optional research role (Perplexity)**\n  - Implement a separate orchestration path that calls Perplexity (or other provider) for research/ranking/title/aux prompts.\n  - If research role is requested but not configured (no model or no key), the SSE stream to the frontend must emit a typed error event (see Task 4) indicating research functionality is unavailable.\n\n- **Security & robustness**\n  - Never log full API keys; at most log provider names and last 4 chars of keys when debugging.\n  - Keep provider-specific request/response normalization inside the abstraction layer to minimize leakage of provider quirks into the main app.\n\n- **Future-proofing**\n  - Store model names as plain strings; do not hard-code the list of allowed model strings beyond provider type checking, so upgrades to `gpt-5` or new Claude versions are configuration-only changes.\n\n<info added on 2025-12-09T15:31:27.345Z>\n**Implementation completed (commit 71891d7):**\n\n- **File structure created:**\n  - `backend/providers/parser.py` - Provider:model notation parser with validation\n  - `backend/providers/base.py` - Abstract `LLMProviderClient` base class\n  - `backend/providers/openai_provider.py` - OpenAI GPT models implementation\n  - `backend/providers/anthropic_provider.py` - Claude models with proper message format\n  - `backend/providers/gemini_provider.py` - Google Gemini with content/parts conversion\n  - `backend/providers/perplexity_provider.py` - OpenAI-compatible implementation\n  - `backend/providers/openrouter_provider.py` - Multi-provider proxy\n  - `backend/providers/registry.py` - Provider registry with env-based config and caching\n  - `backend/llm_client.py` - High-level interface replacing openrouter.py\n\n- **Configuration updates:**\n  - Updated `backend/config.py` to parse `COUNCIL_MODELS`, `CHAIRMAN_MODEL`, `RESEARCH_MODEL`\n  - Added comma-separated model list support\n  - Implemented startup validation with fail-fast behavior for missing API keys\n  - Created comprehensive `.env.example` with provider:model notation examples\n\n- **Integration points:**\n  - Updated `backend/council.py` to use new provider abstraction\n  - Added startup validation hook in `backend/main.py`\n  - All providers use async httpx with configurable timeouts\n  - Research model optional with graceful degradation\n\n- **Technical implementation notes:**\n  - Used Python instead of Node.js/TypeScript as indicated by file extensions\n  - All HTTP calls use async httpx instead of node-fetch/undici\n  - Error handling converts provider exceptions to descriptive messages\n  - Provider-specific quirks isolated within abstraction layer\n  - Client caching prevents duplicate provider instantiation\n</info added on 2025-12-09T15:31:27.345Z>",
        "testStrategy": "- **Unit tests**\n  - Test `parseProviderModel` with valid/invalid strings, unknown providers, missing parts.\n  - Test env parsing for council/chairman/research models, including multiple council models and no research model.\n  - Mock env vars to verify startup validation fails when a configured provider lacks its API key, and passes when present.\n  - For each provider client (OpenAI, Anthropic, Gemini, Perplexity, OpenRouter), mock HTTP and assert correct URLs, headers, and payload formats are used.\n  - Test `callRoleLLM` routes to the correct provider and model based on role and env.\n\n- **Integration tests**\n  - With fake or sandbox keys, issue a full council request and ensure each configured role calls the correct provider endpoint.\n  - Configure an invalid provider string and verify the service refuses to start with a clear error.\n  - Configure `RESEARCH_MODEL` without its API key and verify research role is marked unavailable and surfaces a structured error when invoked.\n\n- **Negative-path tests**\n  - Attempt to call a role with no configured model and assert an immediate, descriptive error is thrown and translated to SSE error events later.\n  - Simulate provider HTTP 401/403 and ensure they are surfaced as provider-specific failures (to be wired into SSE in Task 4).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T15:31:29.483Z"
      },
      {
        "id": "2",
        "title": "Storage safety: UUID conversation IDs, path sanitization, and response size capping",
        "description": "Harden conversation storage by enforcing UUID IDs at API boundary, sanitizing file paths to remain under data/conversations/, and truncating oversized responses before writing to disk.",
        "details": "Implementation outline:\n\n- **UUID conversation IDs at API boundary**\n  - Standardize conversation identifier as **UUID v4** strings.\n  - Use a robust UUID library: e.g., `uuid@9` (`import { validate, version } from 'uuid';`).\n  - On any API endpoint that accepts a conversation ID (read/write/list/delete), validate:\n    ```ts\n    function assertValidConversationId(id: string) {\n      if (!validate(id) || version(id) !== 4) {\n        throw new BadRequestError('conversation_id must be a valid UUID v4');\n      }\n    }\n    ```\n  - Reject non-UUIDs before any filesystem or DB access and return HTTP 400 with a clear JSON error.\n\n- **Safe path construction**\n  - Ensure all conversation files are stored under `data/conversations/`.\n  - Use a single helper to resolve the path and prevent traversal:\n    ```ts\n    import path from 'node:path';\n\n    const CONVERSATION_ROOT = path.resolve('data/conversations');\n\n    function getConversationFilePath(conversationId: string): string {\n      assertValidConversationId(conversationId);\n      const safeName = `${conversationId}.json`;\n      const fullPath = path.resolve(CONVERSATION_ROOT, safeName);\n      if (!fullPath.startsWith(CONVERSATION_ROOT + path.sep)) {\n        throw new Error('Invalid conversation path');\n      }\n      return fullPath;\n    }\n    ```\n  - Create directories on startup if missing (using `fs.mkdir(CONVERSATION_ROOT, { recursive: true })`).\n\n- **Response size capping / truncation**\n  - Define a maximum stored response size in bytes or characters, configurable via env, e.g. `MAX_STORED_RESPONSE_BYTES` with a sensible default (e.g., 256KB–1MB).\n  - Before persisting an LLM turn, apply truncation to large assistant messages:\n    ```ts\n    const MAX_BYTES = Number(process.env.MAX_STORED_RESPONSE_BYTES ?? 262144);\n\n    function truncateForStorage(text: string): string {\n      const encoder = new TextEncoder();\n      const data = encoder.encode(text);\n      if (data.byteLength <= MAX_BYTES) return text;\n      const truncated = data.slice(0, MAX_BYTES);\n      const decoder = new TextDecoder('utf-8', { fatal: false });\n      return decoder.decode(truncated) + '\\n[TRUNCATED]';\n    }\n    ```\n  - Apply truncation only to the **persisted representation**; stream full content to the client as permitted by other limits.\n\n- **File format & schema**\n  - Store each conversation as JSON with a clear schema, e.g.:\n    ```json\n    {\n      \"id\": \"<uuid-v4>\",\n      \"createdAt\": \"ISO-8601\",\n      \"updatedAt\": \"ISO-8601\",\n      \"messages\": [\n        {\"id\":\"uuid-v4\",\"role\":\"user\",\"content\":\"...\",\"createdAt\":\"...\"},\n        {\"id\":\"uuid-v4\",\"role\":\"assistant\",\"content\":\"...\",\"createdAt\":\"...\"}\n      ]\n    }\n    ```\n  - Validate this structure when reading; if corrupted, handle gracefully (return 500 + message) rather than crashing.\n\n- **Operational safeguards**\n  - Keep `data/` in `.gitignore` (Task 6 will update docs, but implementation must not rely on Git only).\n  - Optionally add a background script to detect and log oversized or corrupted files.\n\n- **Performance considerations**\n  - Prefer streaming writes or append strategies only after truncation; avoid holding extremely large responses in memory when not needed.\n  - Use asynchronous file operations (`fs.promises`) to keep the event loop responsive.\n\n<info added on 2025-12-09T15:42:16.174Z>\n**Implementation Status: COMPLETE**\n\nAll core storage safety features have been successfully implemented in Python backend:\n\n- **Storage utilities module** (`backend/storage_utils.py`):\n  - `validate_conversation_id()` - UUID v4 validation with canonical string format enforcement\n  - `get_safe_conversation_path()` - Path traversal prevention using Path.resolve() and relative_to()\n  - `truncate_for_storage()` - UTF-8 safe byte-based truncation with [TRUNCATED] marker\n  - Custom exceptions: `InvalidConversationIdError`, `PathTraversalError`\n\n- **Storage layer hardening** (`backend/storage.py`):\n  - UUID v4 validation applied to all conversation_id parameters before file operations\n  - Safe path construction integrated into `get_conversation_path()`\n  - Graceful corrupted file handling in `get_conversation()` - returns None instead of crashing\n  - Response truncation implemented in `add_assistant_message()` for all 3 processing stages\n  - `MAX_STORED_RESPONSE_BYTES` environment configuration (default 256KB)\n\n- **API boundary protection** (`backend/main.py`):\n  - Exception handling for `InvalidConversationIdError` and `PathTraversalError`\n  - HTTP 400 responses for invalid UUIDs and path traversal attempts\n  - Applied to all conversation endpoints: `get_conversation`, `send_message`, `send_message_stream`\n\n- **Configuration documentation**:\n  - Added `MAX_STORED_RESPONSE_BYTES` to `.env.example` with usage documentation\n\n**Security posture achieved**:\n- UUID v4 validation prevents injection attacks and ensures canonical identifier format\n- Path resolution with relative_to() check prevents directory traversal attacks\n- Response size capping protects against disk exhaustion from oversized LLM outputs\n- Graceful JSON corruption handling prevents application crashes\n- All validation occurs at API boundary before any storage operations\n\nImplementation completed and committed in 444c774.\n</info added on 2025-12-09T15:42:16.174Z>",
        "testStrategy": "- **Unit tests**\n  - Test `assertValidConversationId` with valid/invalid UUIDs, ensuring non-v4 or malformed IDs are rejected.\n  - Test `getConversationFilePath`:\n    - For a valid UUID, path is under `data/conversations/` and ends with `.json`.\n    - For crafted traversal IDs (e.g., `\"../evil\"`), ensure validation rejects before path resolution.\n  - Test `truncateForStorage` with responses below and above threshold, verifying byte-based truncation and `[TRUNCATED]` suffix.\n\n- **Integration tests**\n  - Create a conversation via API using a valid UUID; then verify that a JSON file exists at `data/conversations/<uuid>.json`.\n  - Try to access a conversation with `conversation_id=123` or `\"../../etc/passwd\"` and assert HTTP 400 with appropriate error body.\n  - Simulate a very large assistant response and check that:\n    - The client receives the full stream (if allowed by other limits).\n    - The stored file contains a truncated version with the marker.\n\n- **Resilience tests**\n  - Manually corrupt a stored JSON file and verify read operations handle it by returning an error without crashing the process.\n  - Benchmark typical write paths to confirm truncation and validation do not significantly degrade throughput for normal-sized conversations.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-12-09T15:42:18.721Z"
      },
      {
        "id": "3",
        "title": "Auth and rate limiting for write endpoints",
        "description": "Add optional shared-secret authentication on write endpoints and a simple configurable per-token/IP rate limiter, disabled by default for local use.",
        "details": "Implementation outline:\n\n- **Identify write endpoints**\n  - Treat endpoints that create/modify data as **write** (e.g., start conversation, send message, delete conversation, rename, etc.).\n  - Read-only (fetch conversation, health, config) remain unauthenticated unless otherwise needed.\n\n- **Shared secret header auth**\n  - Introduce env-configured shared secret, e.g., `SHARED_WRITE_TOKEN`.\n  - Choose a header name like `X-Shared-Token`.\n  - Middleware pseudo-code:\n    ```ts\n    const WRITE_TOKEN = process.env.SHARED_WRITE_TOKEN;\n\n    function sharedSecretMiddleware(req, res, next) {\n      if (!WRITE_TOKEN) return next(); // feature disabled\n\n      const token = req.header('X-Shared-Token');\n      if (!token || token !== WRITE_TOKEN) {\n        return res.status(401).json({ error: 'Unauthorized: invalid or missing shared token' });\n      }\n      return next();\n    }\n    ```\n  - Apply only to **write routes**.\n  - Store token only in env; never log it.\n\n- **Per-token/IP rate limiting**\n  - Add env toggle: `RATE_LIMIT_ENABLED` (default `false`) and settings like `RATE_LIMIT_WINDOW_MS`, `RATE_LIMIT_MAX_REQUESTS`.\n  - Implement an in-memory sliding window or fixed window counter suitable for single-node deployments (direct path, no distributed infra).\n  - Keying strategy:\n    - Prefer token when present: `key = 'token:' + token`.\n    - Fallback to IP: `key = 'ip:' + req.ip`.\n  - Pseudo-code:\n    ```ts\n    interface RateEntry { count: number; resetAt: number; }\n    const rateStore = new Map<string, RateEntry>();\n\n    function rateLimitMiddleware(req, res, next) {\n      if (process.env.RATE_LIMIT_ENABLED !== 'true') return next();\n\n      const windowMs = Number(process.env.RATE_LIMIT_WINDOW_MS ?? 60000);\n      const maxReq = Number(process.env.RATE_LIMIT_MAX_REQUESTS ?? 60);\n\n      const token = req.header('X-Shared-Token');\n      const key = token ? `t:${token}` : `ip:${req.ip}`;\n      const now = Date.now();\n      const entry = rateStore.get(key) ?? { count: 0, resetAt: now + windowMs };\n\n      if (now > entry.resetAt) {\n        entry.count = 0;\n        entry.resetAt = now + windowMs;\n      }\n\n      entry.count += 1;\n      rateStore.set(key, entry);\n\n      if (entry.count > maxReq) {\n        res.setHeader('Retry-After', Math.ceil((entry.resetAt - now)/1000));\n        return res.status(429).json({ error: 'Rate limit exceeded' });\n      }\n\n      res.setHeader('X-RateLimit-Remaining', Math.max(0, maxReq - entry.count));\n      return next();\n    }\n    ```\n  - Middleware order for write routes: `sharedSecret -> rateLimit -> handler`.\n\n- **Defaults for local use**\n  - Document that `SHARED_WRITE_TOKEN` unset and `RATE_LIMIT_ENABLED=false` yields behavior equivalent to current system (no auth, no limiting).\n  - Provide safe defaults (e.g., `60 req/min`) when rate limiting is enabled without explicit settings.\n\n- **Security considerations**\n  - Encourage HTTPS termination in deployment docs (Task 6), even though implementation remains protocol-agnostic.\n  - Consider trusting `X-Forwarded-For` only when behind known proxies; keep IP detection simple in code but document caveats.\n\n- **Extensibility**\n  - Abstract rate limit store so a Redis-based implementation can be plugged in later without changing middleware signature.\n",
        "testStrategy": "- **Unit tests**\n  - For `sharedSecretMiddleware`:\n    - With no `SHARED_WRITE_TOKEN`, verify all requests pass through.\n    - With token set, verify:\n      - Missing header → 401.\n      - Wrong token → 401.\n      - Correct token → next() called.\n  - For `rateLimitMiddleware` with in-memory store:\n    - With `RATE_LIMIT_ENABLED` not `true`, verify pass-through.\n    - With enabled, simulate more than `RATE_LIMIT_MAX_REQUESTS` within a window and expect 429 and `Retry-After` header.\n    - Simulate crossing the window boundary and ensure counters reset.\n\n- **Integration tests**\n  - Attach both middlewares to write endpoints and:\n    - Hit endpoint without token when required → 401.\n    - Hit with correct token repeatedly until hitting configured limit → last one returns 429.\n    - Verify `X-RateLimit-Remaining` decrements as expected.\n  - Confirm that read-only endpoints remain accessible without token and are not rate limited.\n\n- **Security tests**\n  - Ensure logs do not include the shared secret by searching log output for token values in test runs.\n  - Fuzz header parsing with long or malformed token strings to ensure no crashes or excessive memory usage.",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "in-progress",
        "subtasks": [],
        "updatedAt": "2025-12-09T16:10:17.460Z"
      },
      {
        "id": "4",
        "title": "Resilience, retries, structured logging, SSE error propagation, and health/config endpoint",
        "description": "Improve upstream resilience with retries and tuned timeouts, implement structured logging, propagate provider failures to the frontend via SSE error events, and add an optional health/config endpoint that exposes enabled providers without secrets.",
        "details": "Implementation outline:\n\n- **Retries with backoff for upstream LLM calls**\n  - In the provider abstraction (Task 1), wrap outbound HTTP calls with retry logic for **idempotent** operations (LLM generation is not strictly idempotent, but for robustness limited retries are acceptable on network errors/timeouts).\n  - Use exponential backoff with jitter; e.g., `p-retry@6` or a custom implementation:\n    ```ts\n    async function withRetries<T>(fn: () => Promise<T>, opts: { retries: number; baseDelayMs: number }): Promise<T> {\n      let attempt = 0;\n      while (true) {\n        try {\n          return await fn();\n        } catch (err) {\n          attempt++;\n          if (attempt > opts.retries) throw err;\n          const delay = opts.baseDelayMs * 2 ** (attempt - 1) * (0.5 + Math.random());\n          await new Promise(r => setTimeout(r, delay));\n        }\n      }\n    }\n    ```\n  - Apply a small number of retries (e.g., 2–3) on transient errors (ECONNRESET, ETIMEDOUT, 5xx). Do **not** retry on 4xx.\n\n- **Shorter timeouts for title generation**\n  - Title and auxiliary prompts should use more aggressive timeouts (e.g., 3–5s) versus main conversation turns (e.g., 30–60s).\n  - In each provider client, accept a `timeoutMs` override. For title generation functions, call providers with the shorter timeout; on timeout, surface a non-fatal error and skip title update rather than failing the whole conversation.\n\n- **Structured logging**\n  - Adopt a structured logger such as `pino@9` or `winston@3`; prefer `pino` for performance.\n  - Standardize log fields:\n    - `level` (`info`, `warn`, `error`)\n    - `timestamp`\n    - `requestId` (attach from middleware using `x-request-id` or generated UUID)\n    - `conversationId`, `provider`, `model`, `role`\n    - `errorCode`, `statusCode` for failures.\n  - Example usage:\n    ```ts\n    logger.info({ provider, model, role }, 'LLM request start');\n    logger.error({ provider, model, role, err }, 'LLM request failed');\n    ```\n  - Ensure secrets (API keys, shared tokens) are filtered via logger serializers or redaction settings.\n\n- **SSE error propagation**\n  - For endpoints that stream via **Server-Sent Events (SSE)**, define a consistent error event protocol:\n    - Event name: `error`\n    - Data payload JSON, e.g.:\n      ```json\n      {\n        \"type\": \"provider_error\",\n        \"provider\": \"openai\",\n        \"message\": \"Upstream timeout\",\n        \"retryable\": true\n      }\n      ```\n  - Pseudo-code for sending errors:\n    ```ts\n    function sseSend(res, event, data) {\n      res.write(`event: ${event}\\n`);\n      res.write(`data: ${JSON.stringify(data)}\\n\\n`);\n    }\n\n    function handleLLMError(res, err, ctx) {\n      const payload = {\n        type: 'provider_error',\n        provider: ctx.provider,\n        message: err.message,\n        retryable: isRetryable(err),\n      };\n      sseSend(res, 'error', payload);\n    }\n    ```\n  - Use this for:\n    - Provider unavailability / missing config (from Task 1 fail-fast checks when invoked at runtime).\n    - Network timeouts and 5xx.\n    - Research role unavailable (explicit error type like `research_unavailable`).\n\n- **Health/config endpoint**\n  - Add optional endpoint, e.g. `GET /health` or `GET /config/providers`.\n  - Response content should **not** leak secrets, only high-level configuration like enabled providers and which roles are active:\n    ```json\n    {\n      \"status\": \"ok\",\n      \"providers\": {\n        \"openai\": {\"enabled\": true},\n        \"anthropic\": {\"enabled\": false},\n        \"gemini\": {\"enabled\": true},\n        \"perplexity\": {\"enabled\": false},\n        \"openrouter\": {\"enabled\": true}\n      },\n      \"roles\": {\n        \"council\": [\"openai:gpt-4.1\", \"openrouter:gpt-4.1\"],\n        \"chairman\": \"anthropic:claude-3-5-sonnet\",\n        \"research\": null\n      }\n    }\n    ```\n  - Add env flag `EXPOSE_HEALTH_ENDPOINT=true|false` to guard exposure in sensitive environments.\n\n- **Observability hooks**\n  - Optionally emit basic metrics counters (e.g., Prometheus-style) for requests per provider, failures, and latency percentiles, but keep implementation minimal: e.g., capture timestamps and compute simple averages for logging.\n\n- **Error taxonomy**\n  - Define internal error classes (e.g., `ProviderConfigError`, `ProviderTimeoutError`, `ProviderRateLimitError`) and map them uniformly to SSE error payloads and HTTP error responses.\n",
        "testStrategy": "- **Unit tests**\n  - Test `withRetries` with a function that fails N times then succeeds; verify expected number of attempts and delay behavior (use fake timers).\n  - Ensure non-retryable errors (e.g., 4xx) are not retried.\n  - Test provider clients honor `timeoutMs` by aborting requests and throwing a timeout error.\n  - Test logger integration: ensure log entries include `provider`, `model`, `role`, and no secrets.\n  - Test SSE helper `sseSend` formats messages correctly for different event names.\n\n- **Integration tests**\n  - Simulate upstream LLM timeouts and 5xx using an HTTP mock server; confirm retries happen, then an SSE `error` event is emitted and connection is cleanly closed.\n  - Force a misconfiguration (missing API key for referenced provider) and verify that an SSE `error` with type `provider_error` or `config_error` is sent when that role is invoked.\n  - Call health/config endpoint with various env configurations and assert secrets are not present while enabled/disabled states are accurate.\n\n- **Load & resilience tests**\n  - Under moderate load, confirm retry behavior does not overwhelm upstream services (limit retries) and that logging remains performant (e.g., using `pino` in async mode).\n  - Verify that title generation timeouts do not impact the main response path: users still receive conversation replies even if titles fail.",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "5",
        "title": "Frontend and backend data safety messaging",
        "description": "Add clear data safety messaging explaining that data/conversations/ stores unencrypted local JSON, ensure data/ remains git-ignored, and implement a frontend banner or notice linking to storage warnings and configuration notes.",
        "details": "Implementation outline:\n\n- **Backend messaging source**\n  - Provide a low-friction way to surface data safety messaging to the frontend, e.g. via a small config endpoint or embedding in an existing bootstrap/config API:\n    ```json\n    {\n      \"storage\": {\n        \"path\": \"data/conversations/\",\n        \"encrypted\": false,\n        \"description\": \"Conversations are stored as unencrypted JSON files on local disk.\",\n        \"docsPath\": \"#storage-safety\"\n      }\n    }\n    ```\n  - Hard-code that conversations are **unencrypted local JSON**; do not attempt automatic disk encryption.\n\n- **Ensure .gitignore covers data/**\n  - Verify `.gitignore` includes at least:\n    ```\n    data/\n    data/*\n    ```\n  - If repo uses language-specific templates, append `data/` and ensure `data/conversations/` is not accidentally committed.\n\n- **Frontend banner/notice**\n  - Add a non-intrusive but visible banner to the main app view (e.g., top of page or settings panel):\n    - Text along the lines of: \"Conversations are stored as **unencrypted JSON files** under `data/conversations/` on this machine. Do not use for sensitive data.\"\n    - Include a \"Learn more\" link that opens a modal or navigates to a help page/section describing storage behavior and configuration (e.g., ability to change paths, retention, etc., if supported).\n  - Allow banner dismissal per user (localStorage flag) but make it rediscoverable via a help icon or settings.\n\n- **UI implementation sketch** (React example):\n    ```tsx\n    const STORAGE_DISMISSED_KEY = 'storageWarningDismissed';\n\n    function StorageWarningBanner() {\n      const [dismissed, setDismissed] = useState(() => localStorage.getItem(STORAGE_DISMISSED_KEY) === '1');\n      if (dismissed) return null;\n      return (\n        <div className=\"storage-banner\">\n          <p>\n            Conversations are stored as <strong>unencrypted JSON files</strong> under <code>data/conversations/</code> on this machine.\n            Avoid storing sensitive or personal data.\n          </p>\n          <button onClick={() => { localStorage.setItem(STORAGE_DISMISSED_KEY, '1'); setDismissed(true); }}>Dismiss</button>\n          <button onClick={openStorageDetails}>Learn more</button>\n        </div>\n      );\n    }\n    ```\n\n- **Storage warning details**\n  - In the detailed UI (modal or settings page), explain:\n    - Storage location: `data/conversations/`.\n    - Data format: unencrypted JSON.\n    - That deleting the folder deletes conversation history.\n    - That encryption, off-disk storage, or rotation are out of scope unless separately configured.\n\n- **Configuration notes linkage**\n  - The \"Learn more\" component should reference documentation about:\n    - Provider configuration.\n    - Auth/rate limiting.\n    - SSE error behaviors.\n  - Implementation can navigate to an in-app \"Help / Docs\" route or open an embedded markdown page.\n",
        "testStrategy": "- **Unit tests (frontend)**\n  - Test `StorageWarningBanner` renders when `storageWarningDismissed` is not set and hides after clicking Dismiss.\n  - Verify that the \"Learn more\" button triggers navigation or modal opening as expected.\n\n- **Unit tests (backend)**\n  - If a storage/config endpoint is exposed, test that it reports `encrypted: false` and the correct storage path.\n\n- **Integration tests**\n  - End-to-end UI test (e.g., Playwright/Cypress):\n    - First load: banner is visible, with correct text mentioning `data/conversations/` and unencrypted JSON.\n    - After dismissing and reloading, banner is hidden.\n  - Verify `.gitignore` is present in the repo and contains `data/` entries; as a CI safeguard, optionally run a script that fails if any file under `data/` is tracked by Git in test runs.\n\n- **Accessibility and UX checks**\n  - Confirm banner has sufficient contrast, is screen-reader friendly, and is dismissible via keyboard.",
        "priority": "medium",
        "dependencies": [
          "2",
          "4"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "6",
        "title": "Documentation and configuration updates for providers, auth, rate limits, SSE errors, and storage",
        "description": "Update .env.example, README, and UI/docs to describe provider keys and notation, shared secret auth, rate limit toggles, SSE error handling expectations, and storage safety behavior.",
        "details": "Implementation outline:\n\n- **.env.example updates**\n  - Add placeholders and comments for all provider keys:\n    ```env\n    # LLM provider API keys\n    OPENAI_API_KEY=\n    ANTHROPIC_API_KEY=\n    GEMINI_API_KEY=\n    PERPLEXITY_API_KEY=\n    OPENROUTER_API_KEY=\n\n    # Model configuration using provider:model notation\n    # e.g. openai:gpt-4.1, anthropic:claude-3-5-sonnet, gemini:gemini-2.0-pro, perplexity:sonar-pro, openrouter:gpt-4.1\n    COUNCIL_MODELS=openai:gpt-4.1,anthropic:claude-3-5-sonnet\n    CHAIRMAN_MODEL=anthropic:claude-3-5-sonnet\n    RESEARCH_MODEL=perplexity:sonar-pro\n\n    # Optional shared secret for write endpoints\n    SHARED_WRITE_TOKEN=\n\n    # Rate limiting (default off for local use)\n    RATE_LIMIT_ENABLED=false\n    RATE_LIMIT_WINDOW_MS=60000\n    RATE_LIMIT_MAX_REQUESTS=60\n\n    # Response storage caps\n    MAX_STORED_RESPONSE_BYTES=262144\n\n    # Health/config endpoint exposure\n    EXPOSE_HEALTH_ENDPOINT=true\n    ```\n\n- **README provider & routing docs**\n  - Add a section explaining **provider:model notation**:\n    - Syntax: `provider:model`.\n    - Supported providers and examples: `openai:gpt-4.1`, `anthropic:claude-3-5-sonnet`, `gemini:gemini-2.0-pro`, `perplexity:sonar-pro`, `openrouter:gpt-4.1`.\n    - How to configure council list and chairman model via env.\n  - Document each provider’s required env key and any special notes (e.g., OpenRouter uses OpenAI-compatible payloads but different base URL).\n  - Explain optional **research role** usage and its dependency on Perplexity (or other configured provider) and how errors will surface if it is missing.\n\n- **Auth & rate limiting docs**\n  - Describe how to enable the shared secret header:\n    - Set `SHARED_WRITE_TOKEN`.\n    - Include `X-Shared-Token: <value>` on all write requests.\n  - Explain rate limiting behavior:\n    - How per-token/IP keys are determined.\n    - Default limits and how to override window/maximums.\n    - Emphasize that `RATE_LIMIT_ENABLED=false` is recommended for local development.\n\n- **Resilience & SSE error handling docs**\n  - Explain that upstream LLM calls use retries with exponential backoff on transient failures and may still produce SSE `error` events if unrecoverable.\n  - Document SSE error event format and expectations for frontend consumers, including example payloads for:\n    - `provider_error`\n    - `config_error`\n    - `research_unavailable`\n  - Note that title generation uses shorter timeouts and failure only affects titles, not main responses.\n\n- **Storage safety documentation**\n  - Add a prominent section \"Data Storage & Safety\":\n    - Conversations are stored as **unencrypted JSON** under `data/conversations/`.\n    - `data/` is `.gitignore`d and should not be checked into version control.\n    - Recommend not storing sensitive or regulated data.\n    - Outline how to clear data by deleting the folder.\n  - Link this section from the UI banner (Task 5).\n\n- **Config & health endpoint docs**\n  - Document the health/config endpoint path, what it returns (enabled providers, roles), and how to enable/disable it via `EXPOSE_HEALTH_ENDPOINT`.\n  - Emphasize that no secrets are exposed and that this endpoint is intended for local diagnostics and observability.\n\n- **Developer guidance**\n  - Briefly describe the provider abstraction and how to add new providers or models by configuration only.\n  - Include examples of typical `.env` setups for:\n    - Single-provider local dev (only OpenAI).\n    - Multi-provider council (OpenAI + Anthropic + Gemini).\n    - Research-enabled setup with Perplexity.\n\n- **Consistency checks**\n  - Ensure variable names and routes used in documentation exactly match implementation.\n  - Add a lightweight CI check (optional) to ensure `.env.example` and README remain in sync with required env vars (e.g., a script that scans for `process.env.*` and ensures they are mentioned in `.env.example`).\n",
        "testStrategy": "- **Documentation review**\n  - Peer review README and `.env.example` for accuracy and clarity, cross-checking with actual code (env variable names, endpoints, header names).\n\n- **Automated checks (optional but recommended)**\n  - Implement a small test script that:\n    - Parses the codebase for `process.env.*` usages.\n    - Verifies each appears in `.env.example`.\n  - Run this script as part of CI, failing if discrepancies are found.\n\n- **Manual validation**\n  - Follow README setup steps on a fresh clone to ensure the app can be configured and run using only the documented instructions.\n  - Verify that SSE error examples in docs reflect actual runtime payloads by capturing a failing scenario and comparing.\n\n- **UX validation**\n  - Confirm that links from the UI banner or settings to the storage/config documentation sections are correct and not broken.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5"
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-09T16:10:17.466Z",
      "taskCount": 6,
      "completedCount": 2,
      "tags": [
        "master"
      ]
    }
  }
}